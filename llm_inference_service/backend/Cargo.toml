[package]
name = "llm_inference_service"
version = "0.1.0"
edition = "2024"

[dependencies]
candle-core = { version = "0.8.2", features = ["cuda"] }
candle-nn = { version = "0.8.2", features = ["cuda"] }
candle-transformers = { version = "0.8.2", features = ["cuda"] }
hf-hub = "0.4.1"
tokenizers = "0.21.0"
anyhow = "1.0"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
axum = "0.7"
tokio = { version = "1", features = ["full"] }
tokio-stream = "0.1"
tower-http = { version = "0.5", features = ["cors"] }

